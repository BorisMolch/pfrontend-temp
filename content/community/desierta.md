+++
author = "CNDSD & Ivan Abreu"
bio = ""
date = 2021-05-16T22:00:00Z
description = "**Audiovisual Concert**\n\n**VR + Live Cinema Coding**\n\n_Desierta_ (deserted) takes place in tunnels of London and CDMX, empty subways and deserted cities. The spaces are Creative Commons-licensed scanned and 3D models, which were optimized, modified, and inserted in Patch and in a second 3D environment in the Unity videogame engine. These two spaces become the 3D stages for the concert. Both spaces are mixed with real-time digital composition controlled with live coding. The concert refers to the rarefied feeling of the city depopulated during the pandemic crisis.\n\nThe live act _Desierta_ was the first experimentation in a hybrid concert, a live audiovisual duo of two performers in two universes: the metaverse and the textuality of live coding, which combined two performance processes that generally impregnate different qualities to the music. There are two core elements:\n\n1. The power of Tidal Cycles pattern syntax to abstract sound time with the weightlessness and agility of textuality, impregnating asymmetric and granular progressions to the music. This is characteristic of the style of the compositions of CNDSD (aka Malitzin Cortes)\n2. The gestures of the avatar, activating and patching Patch XR's VR modular synths and sequencers, adding an aesthetic layer full of granular evolutions and patterns of greater symmetry.\n\nMusically, Malitzin played various instruments that she created on Patch.XR and composed the music on Tidal Cycles; Ivan Abreu performed Malitzin's music live.\n\nVisually, Malitzin and digital artist Mariana Mena created and integrated 3D environments. Ivan Abreu programmed and performed the visual narrative live with the various visual sources -- Patch's VR viewfinder camera, a series of engine camera cinematics of Unity videogames, and some Resolume visual effects.\n\nThe stacking of platforms allows visual expression to benefit from live coding since the visual resources are related to the logic of the composition, becoming visualizations of sound algorithms rather than audio-reactive visuals. We call this process “live cinema coding.” For this purpose we have developed a library of functions in Tidal Cycles and a Processing server based on the OSC protocol called “Delivery Bridge Tidal Cycles” that allows manipulating shaders, blending videos, etc. With this we mix the video sources of Patch and the generative real-time graphics of Processing and Unity. Finally, the “live script” in real-time is the musical patterns of Tidal Cycles.\n\nIllustrative (simplified) example:\n\ndo\n\n  d1 $ sound \"Bass * 2\" # gain 0.8\n\n  d2 $ composition \"Subway * 2\" # effects “threshold: opacity_1_0_80”\n\nThe previous Haskell code inside Tidal Cycles uses two layers: d1 and d2; d1 triggers the Bass sample twice in each cycle of the musical time since multiplying this by two \"sample * 2\", at the same time d2 also executes the Subway digital video composition effect twice, since it also multiplies by two  \"subway * 2\". A code like this generates synesthesia and simultaneity between the visual and the sound."
draft = true
image = "/uploads/unnamed.jpg"
item = "patch"
link1 = "https://patchxr.com/blog/cndsd-ivan-abreau-expanding-mind-self-and-music-in-digital-realms/"
link2 = "https://cdm.link/2021/05/futuristic-av-eps-from-mexicos-interspecifics-cndsd-ivan/"
location = "Mexico City"
title = "Desierta"
twitter = "https://twitter.com/CNDSD_"
type = ""
video = "https://vimeo.com/497847561"

+++
